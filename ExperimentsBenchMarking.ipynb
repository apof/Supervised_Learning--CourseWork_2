{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import utilities\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import AdaBoost\n",
    "import KernelPerceptron\n",
    "%pylab inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "height = width = 16\n",
    "class_number = 10\n",
    "N = 20\n",
    "\n",
    "train_type = 'one_Vs_all'\n",
    "#train_type = 'one_Vs_one'\n",
    "\n",
    "algorithm = 'AdaBoost'\n",
    "#algorithm = 'KernelPerceptron'\n",
    "#algorithm = 'SVM'\n",
    "\n",
    "\n",
    "#kernel_type = 'gaussian'\n",
    "#kernel_type = 'polynomial'\n",
    "kernel_type = ' '\n",
    "\n",
    "#parameters = [1,2,3,4,5,6,7]\n",
    "\n",
    "#parameters = [0.01,0.1,0.6,1,2,5]\n",
    "#parameters = [0.005,0.008,0.01,0.02,0.04]\n",
    "\n",
    "#parameters = [30,40,50,60,70]\n",
    "#parameters = [60,80,100]\n",
    "parameters = [100,200,300]\n",
    "\n",
    "#parameters = [0.1,0.5,1,1.5,2,3,4,5,6]\n",
    "#parameters = [4,5,6]\n",
    "\n",
    "EXPERIMENT_TYPE = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9297, 256)\n",
      "(9297,)\n"
     ]
    }
   ],
   "source": [
    "train_data = 'Datasets/full_dataset.dat'\n",
    "data,labels = utilities.read_data(train_data,width,height)\n",
    "print(data.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data = data[0:1000]\n",
    "#labels = labels[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one_Vs_all training for algorithm AdaBoost with params [' ', 100.0] and random split 1\n",
      "one_Vs_all training for algorithm AdaBoost with params [' ', 200.0] and random split 1\n",
      "one_Vs_all training for algorithm AdaBoost with params [' ', 300.0] and random split 1\n",
      "one_Vs_all training for algorithm AdaBoost with params [' ', 100.0] and random split 2\n",
      "one_Vs_all training for algorithm AdaBoost with params [' ', 200.0] and random split 2\n",
      "one_Vs_all training for algorithm AdaBoost with params [' ', 300.0] and random split 2\n",
      "one_Vs_all training for algorithm AdaBoost with params [' ', 100.0] and random split 3\n",
      "one_Vs_all training for algorithm AdaBoost with params [' ', 200.0] and random split 3\n",
      "one_Vs_all training for algorithm AdaBoost with params [' ', 300.0] and random split 3\n",
      "one_Vs_all training for algorithm AdaBoost with params [' ', 100.0] and random split 4\n",
      "one_Vs_all training for algorithm AdaBoost with params [' ', 200.0] and random split 4\n",
      "one_Vs_all training for algorithm AdaBoost with params [' ', 300.0] and random split 4\n",
      "one_Vs_all training for algorithm AdaBoost with params [' ', 100.0] and random split 5\n",
      "one_Vs_all training for algorithm AdaBoost with params [' ', 200.0] and random split 5\n",
      "one_Vs_all training for algorithm AdaBoost with params [' ', 300.0] and random split 5\n",
      "one_Vs_all training for algorithm AdaBoost with params [' ', 100.0] and random split 6\n",
      "one_Vs_all training for algorithm AdaBoost with params [' ', 200.0] and random split 6\n",
      "one_Vs_all training for algorithm AdaBoost with params [' ', 300.0] and random split 6\n",
      "one_Vs_all training for algorithm AdaBoost with params [' ', 100.0] and random split 7\n",
      "one_Vs_all training for algorithm AdaBoost with params [' ', 200.0] and random split 7\n",
      "one_Vs_all training for algorithm AdaBoost with params [' ', 300.0] and random split 7\n",
      "one_Vs_all training for algorithm AdaBoost with params [' ', 100.0] and random split 8\n",
      "one_Vs_all training for algorithm AdaBoost with params [' ', 200.0] and random split 8\n",
      "one_Vs_all training for algorithm AdaBoost with params [' ', 300.0] and random split 8\n",
      "one_Vs_all training for algorithm AdaBoost with params [' ', 100.0] and random split 9\n",
      "one_Vs_all training for algorithm AdaBoost with params [' ', 200.0] and random split 9\n",
      "one_Vs_all training for algorithm AdaBoost with params [' ', 300.0] and random split 9\n",
      "one_Vs_all training for algorithm AdaBoost with params [' ', 100.0] and random split 10\n",
      "one_Vs_all training for algorithm AdaBoost with params [' ', 200.0] and random split 10\n",
      "one_Vs_all training for algorithm AdaBoost with params [' ', 300.0] and random split 10\n",
      "one_Vs_all training for algorithm AdaBoost with params [' ', 100.0] and random split 11\n",
      "one_Vs_all training for algorithm AdaBoost with params [' ', 200.0] and random split 11\n",
      "one_Vs_all training for algorithm AdaBoost with params [' ', 300.0] and random split 11\n",
      "one_Vs_all training for algorithm AdaBoost with params [' ', 100.0] and random split 12\n",
      "one_Vs_all training for algorithm AdaBoost with params [' ', 200.0] and random split 12\n",
      "one_Vs_all training for algorithm AdaBoost with params [' ', 300.0] and random split 12\n",
      "one_Vs_all training for algorithm AdaBoost with params [' ', 100.0] and random split 13\n",
      "one_Vs_all training for algorithm AdaBoost with params [' ', 200.0] and random split 13\n",
      "one_Vs_all training for algorithm AdaBoost with params [' ', 300.0] and random split 13\n",
      "one_Vs_all training for algorithm AdaBoost with params [' ', 100.0] and random split 14\n",
      "one_Vs_all training for algorithm AdaBoost with params [' ', 200.0] and random split 14\n",
      "one_Vs_all training for algorithm AdaBoost with params [' ', 300.0] and random split 14\n",
      "one_Vs_all training for algorithm AdaBoost with params [' ', 100.0] and random split 15\n",
      "one_Vs_all training for algorithm AdaBoost with params [' ', 200.0] and random split 15\n",
      "one_Vs_all training for algorithm AdaBoost with params [' ', 300.0] and random split 15\n",
      "one_Vs_all training for algorithm AdaBoost with params [' ', 100.0] and random split 16\n",
      "one_Vs_all training for algorithm AdaBoost with params [' ', 200.0] and random split 16\n",
      "one_Vs_all training for algorithm AdaBoost with params [' ', 300.0] and random split 16\n",
      "one_Vs_all training for algorithm AdaBoost with params [' ', 100.0] and random split 17\n",
      "one_Vs_all training for algorithm AdaBoost with params [' ', 200.0] and random split 17\n",
      "one_Vs_all training for algorithm AdaBoost with params [' ', 300.0] and random split 17\n",
      "one_Vs_all training for algorithm AdaBoost with params [' ', 100.0] and random split 18\n",
      "one_Vs_all training for algorithm AdaBoost with params [' ', 200.0] and random split 18\n",
      "one_Vs_all training for algorithm AdaBoost with params [' ', 300.0] and random split 18\n",
      "one_Vs_all training for algorithm AdaBoost with params [' ', 100.0] and random split 19\n",
      "one_Vs_all training for algorithm AdaBoost with params [' ', 200.0] and random split 19\n",
      "one_Vs_all training for algorithm AdaBoost with params [' ', 300.0] and random split 19\n",
      "one_Vs_all training for algorithm AdaBoost with params [' ', 100.0] and random split 20\n",
      "one_Vs_all training for algorithm AdaBoost with params [' ', 200.0] and random split 20\n",
      "one_Vs_all training for algorithm AdaBoost with params [' ', 300.0] and random split 20\n",
      "Mean error and std for parameter: 100 is 7.50268817204301 - 0.5585902489754309\n",
      "Mean error and std for parameter: 200 is 6.43010752688172 - 0.5244071800686687\n",
      "Mean error and std for parameter: 300 is 6.126344086021506 - 0.4173078659840241\n"
     ]
    }
   ],
   "source": [
    "#utilities.get_average_confusions(confusion_error_list)\n",
    "if(EXPERIMENT_TYPE == 1):\n",
    "    \n",
    "    ## N runs for random train test configurations\n",
    "    results = np.zeros((N,len(parameters)))\n",
    "    \n",
    "    #confusion_error_list = []\n",
    "\n",
    "    for run_index in range(0,N):\n",
    "    \n",
    "        train_data,test_data,train_labels,test_labels,_ = utilities.data_split(data,labels,0.2)\n",
    "    \n",
    "        data_per_class_dictionary = utilities.collect_each_class_images(train_data,train_labels,class_number)\n",
    "        pair_datasets = utilities.create_1_VS_1_dataset(data_per_class_dictionary)\n",
    "        #print(\"Paired classifiers number: \" + str(len(pair_datasets)))\n",
    "\n",
    "        #print(train_data.shape)\n",
    "        #print(test_data.shape)\n",
    "        #print(train_labels.shape)\n",
    "        #print(test_labels.shape)\n",
    "        for i in range(0,len(parameters)):\n",
    "        \n",
    "            param = parameters[i]\n",
    "        \n",
    "            param_list = []\n",
    "            param_list = param_list + [kernel_type]\n",
    "            param_list  = param_list + [float(param)]\n",
    "            print(train_type + \" training for algorithm \" + algorithm + \" with params \" + str(param_list) +  \" and random split \" + str(run_index+1))\n",
    "        \n",
    "            models_dict = None\n",
    "            pair_models = None\n",
    "            if(train_type == 'one_Vs_all'):\n",
    "                models_dict = utilities.one_VS_all_training(data_per_class_dictionary,class_number,algorithm,param_list)\n",
    "            else:\n",
    "                pair_models = utilities.one_vs_one_training(pair_datasets,algorithm,param_list)\n",
    "         \n",
    "            test_predictions = None\n",
    "            if(train_type == 'one_Vs_all'):\n",
    "                test_predictions,test_confidence,average_confidence = utilities.one_VS_all_testing(algorithm,models_dict,test_data,test_labels)\n",
    "            else:\n",
    "                test_predictions = utilities.one_vs_one_testing(algorithm,pair_models,test_data,class_number)\n",
    "           \n",
    "            results[run_index][i] = utilities.calculate_accuracy(test_predictions,test_labels)\n",
    "            \n",
    "            #confusion_error = utilities.confusion_matrix(test_predictions,test_labels)\n",
    "            \n",
    "            #confusion_error_list.append(confusion_error)\n",
    "            \n",
    "            \n",
    "    for i in range(results.T.shape[0]):\n",
    "        print(\"Mean error and std for parameter: \" + str(parameters[i]) + \" is \" + str(np.mean(results.T[i])) + \" - \" + str(np.std(results.T[i])))\n",
    "        \n",
    "    #utilities.get_average_confusions(confusion_error_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(EXPERIMENT_TYPE == 2):\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    confusion_error_list = []\n",
    "    \n",
    "    hardest_to_predict_dict = {}\n",
    "    \n",
    "    for run_index in range(0,N):\n",
    "        \n",
    "        print(\"------------------------------------------------\")\n",
    "        \n",
    "        ## make a random split (80%-20%) on the dataset\n",
    "        train_data,final_test_data,train_labels,final_test_labels, test_indexes = utilities.data_split(data,labels,0.2)\n",
    "        \n",
    "        cross_valid_results = np.zeros((5,len(parameters)))\n",
    "    \n",
    "        ## then perform 5-k cross validation to find the best parameter on it\n",
    "        for k in range(5):\n",
    "            ## given a validation round split the training data into training and validation data of the round\n",
    "            validation_indexes,training_indexes = utilities.cross_validation(k,len(train_data),5)\n",
    "            round_train_data,round_train_labels,round_valid_data,round_valid_labels = utilities.get_train_test_set(validation_indexes,training_indexes,train_data,train_labels)\n",
    "            \n",
    "            data_per_class_dictionary = utilities.collect_each_class_images(round_train_data,round_train_labels,class_number)\n",
    "            pair_datasets = utilities.create_1_VS_1_dataset(data_per_class_dictionary)\n",
    "        \n",
    "            ## for every cross validation round check all the parameters\n",
    "            for i in range(0,len(parameters)):\n",
    "                param = parameters[i]\n",
    "                param_list = []\n",
    "                param_list = param_list + [kernel_type]\n",
    "                param_list  = param_list + [float(param)]\n",
    "                #print(train_type + \" training for algorithm \" + algorithm + \" with params \" + str(param_list) +  \" random split \" + str(run_index+1) + \" and cross validation round: \" + str(k + 1))\n",
    "        \n",
    "                models_dict = None\n",
    "                pair_models = None\n",
    "                if(train_type == 'one_Vs_all'):\n",
    "                    models_dict = utilities.one_VS_all_training(data_per_class_dictionary,class_number,algorithm,param_list)\n",
    "                else:\n",
    "                    pair_models = utilities.one_vs_one_training(pair_datasets,algorithm,param_list)\n",
    "         \n",
    "                test_predictions = None\n",
    "                if(train_type == 'one_Vs_all'):\n",
    "                    test_predictions,test_confidence,average_confidence = utilities.one_VS_all_testing(algorithm,models_dict,round_valid_data,round_valid_labels)\n",
    "                else:\n",
    "                    test_predictions = utilities.one_vs_one_testing(algorithm,pair_models,round_valid_data,class_number)\n",
    "            \n",
    "                cross_valid_results[k][i] = utilities.calculate_accuracy(test_predictions,round_valid_labels)\n",
    "    \n",
    "        best_param_list = []\n",
    "        ## find the best parameter of this validation round\n",
    "        ## computing the average error for each parameter for each split\n",
    "        cross_valid_results = cross_valid_results.T\n",
    "        for i in range(cross_valid_results.shape[0]):\n",
    "            best_param_list.append((i,np.mean(cross_valid_results[i])))\n",
    "        \n",
    "        ## get the best result and the parameter for which the result achieved\n",
    "        best_param_list = sorted(best_param_list, key=lambda tup: tup[1],reverse = False)\n",
    "        \n",
    "        best_param = parameters[best_param_list[0][0]]\n",
    "        \n",
    "        #print(\"Best Parameter for run round \" + str(run_index+1) + \" is \" + str(best_param) + \" with error: \" + str(best_param_list[0][1]))\n",
    "        \n",
    "        ## now retrain in full 80% dataset and test for the remaining hidden 20% test set\n",
    "        data_per_class_dictionary = utilities.collect_each_class_images(train_data,train_labels,class_number)\n",
    "        pair_datasets = utilities.create_1_VS_1_dataset(data_per_class_dictionary)\n",
    "        \n",
    "        param = best_param\n",
    "        param_list = []\n",
    "        param_list = param_list + [kernel_type]\n",
    "        param_list  = param_list + [float(param)]\n",
    "        #print(train_type + \" training for algorithm \" + algorithm + \" with params \" + str(param_list) + \" for the full dataset\")\n",
    "        \n",
    "        models_dict = None\n",
    "        pair_models = None\n",
    "        if(train_type == 'one_Vs_all'):\n",
    "            models_dict = utilities.one_VS_all_training(data_per_class_dictionary,class_number,algorithm,param_list)\n",
    "        else:\n",
    "            pair_models = utilities.one_vs_one_training(pair_datasets,algorithm,param_list)\n",
    "         \n",
    "        ## test on the remaining 20% which was hidden through the cross validation\n",
    "        test_predictions = None\n",
    "        if(train_type == 'one_Vs_all'):\n",
    "            test_predictions,test_confidence,average_confidence = utilities.one_VS_all_testing(algorithm,models_dict,final_test_data,final_test_labels)\n",
    "        else:\n",
    "            test_predictions = utilities.one_vs_one_testing(algorithm,pair_models,final_test_data,class_number)\n",
    "         \n",
    "        accuracy = utilities.calculate_accuracy(test_predictions,final_test_labels)\n",
    "        results.append((accuracy,best_param))\n",
    "        \n",
    "        confusion_error_list.append(utilities.confusion_matrix(test_predictions,final_test_labels))\n",
    "        \n",
    "        if(train_type == 'one_Vs_all'):\n",
    "            hard_items_indexes,_ = utilities.get_hardest_to_predict_items(test_confidence,average_confidence,test_predictions,final_test_labels,test_indexes)\n",
    "\n",
    "            ## find the 5 hardest indexes of this round\n",
    "            hardest_to_predict_dict[run_index] = hard_items_indexes\n",
    "        \n",
    "            utilities.print_hard_items(hard_items_indexes,final_test_data)\n",
    "    \n",
    "    \n",
    "    ## based on the 5 hardest to be predicted itemss of every round\n",
    "    ## find the hardest to predict items overall\n",
    "    utilities.get_hardest_to_predict_items_overall(hardest_to_predict_dict,data)\n",
    "        \n",
    "    print(results)\n",
    "    \n",
    "    error = []\n",
    "    param = []\n",
    "    for (e,p) in results:\n",
    "        error.append(e)\n",
    "        param.append(p)\n",
    "        \n",
    "    print(\"--------\")\n",
    "\n",
    "        \n",
    "    print(\"Mean error: \" + str(np.mean(error)))\n",
    "    print(\"Std error: \" + str(np.std(error)))\n",
    "    print(\"Mean param: \" + str(np.mean(param)))\n",
    "    print(\"Std param: \" + str(np.std(param)))\n",
    "\n",
    "    \n",
    "    print(\"--------\")\n",
    "    \n",
    "    utilities.get_average_confusions(confusion_error_list)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "imgplot = plt.imshow(data[8260].reshape((16,16)))\n",
    "plt.show()\n",
    "print(labels[8260])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
