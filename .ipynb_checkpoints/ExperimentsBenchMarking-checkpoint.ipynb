{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import utilities\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import AdaBoost\n",
    "import KernelPerceptron\n",
    "%pylab inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "height = width = 16\n",
    "class_number = 10\n",
    "N = 5\n",
    "\n",
    "train_type = 'one_Vs_all'\n",
    "#train_type = 'one_Vs_one'\n",
    "\n",
    "#algorithm = 'AdaBoost'\n",
    "algorithm = 'KernelPerceptron'\n",
    "#algorithm = 'SVM'\n",
    "\n",
    "\n",
    "kernel_type = 'gaussian'\n",
    "#kernel_type = 'polynomial'\n",
    "#kernel_type = ' '\n",
    "\n",
    "#parameters = [1,2,3,4,5,6,7]\n",
    "#parameters = [0.01,0.1,1]\n",
    "parameters = [0.1,0.2,0.3]\n",
    "#parameters = [30,50,60]\n",
    "#parameters = [0.5,1,1.5]\n",
    "\n",
    "EXPERIMENT_TYPE = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9297, 256)\n",
      "(9297,)\n"
     ]
    }
   ],
   "source": [
    "train_data = 'Datasets/full_dataset.dat'\n",
    "data,labels = utilities.read_data(train_data,width,height)\n",
    "print(data.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[0:1000]\n",
    "labels = labels[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one_Vs_all training for algorithm KernelPerceptron with params ['gaussian', 0.1] and random split 1\n"
     ]
    }
   ],
   "source": [
    "#utilities.get_average_confusions(confusion_error_list)\n",
    "if(EXPERIMENT_TYPE == 1):\n",
    "    \n",
    "    ## N runs for random train test configurations\n",
    "    results = np.zeros((N,len(parameters)))\n",
    "    \n",
    "    #confusion_error_list = []\n",
    "\n",
    "    for run_index in range(0,N):\n",
    "    \n",
    "        train_data,test_data,train_labels,test_labels = utilities.data_split(data,labels,0.2)\n",
    "    \n",
    "        data_per_class_dictionary = utilities.collect_each_class_images(train_data,train_labels,class_number)\n",
    "        pair_datasets = utilities.create_1_VS_1_dataset(data_per_class_dictionary)\n",
    "        #print(\"Paired classifiers number: \" + str(len(pair_datasets)))\n",
    "\n",
    "        #print(train_data.shape)\n",
    "        #print(test_data.shape)\n",
    "        #print(train_labels.shape)\n",
    "        #print(test_labels.shape)\n",
    "        for i in range(0,len(parameters)):\n",
    "        \n",
    "            param = parameters[i]\n",
    "        \n",
    "            param_list = []\n",
    "            param_list = param_list + [kernel_type]\n",
    "            param_list  = param_list + [float(param)]\n",
    "            print(train_type + \" training for algorithm \" + algorithm + \" with params \" + str(param_list) +  \" and random split \" + str(run_index+1))\n",
    "        \n",
    "            models_dict = None\n",
    "            pair_models = None\n",
    "            if(train_type == 'one_Vs_all'):\n",
    "                models_dict = utilities.one_VS_all_training(data_per_class_dictionary,class_number,algorithm,param_list)\n",
    "            else:\n",
    "                pair_models = utilities.one_vs_one_training(pair_datasets,algorithm,param_list)\n",
    "         \n",
    "            test_predictions = None\n",
    "            if(train_type == 'one_Vs_all'):\n",
    "                test_predictions,test_confidence,average_confidence = utilities.one_VS_all_testing(algorithm,models_dict,test_data,test_labels)\n",
    "            else:\n",
    "                test_predictions = utilities.one_vs_one_testing(algorithm,pair_models,test_data,class_number)\n",
    "           \n",
    "            results[run_index][i] = utilities.calculate_accuracy(test_predictions,test_labels)\n",
    "            \n",
    "            #confusion_error = utilities.confusion_matrix(test_predictions,test_labels)\n",
    "            \n",
    "            #confusion_error_list.append(confusion_error)\n",
    "            \n",
    "            \n",
    "    for i in range(results.T.shape[0]):\n",
    "        print(\"Mean error and std for parameter: \" + str(parameters[i]) + \" is \" + str(np.mean(results.T[i])) + \" - \" + str(np.std(results.T[i])))\n",
    "        \n",
    "    #utilities.get_average_confusions(confusion_error_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(EXPERIMENT_TYPE == 2):\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    confusion_error_list = []\n",
    "    \n",
    "    for run_index in range(0,N):\n",
    "        ## make a random split (80%-20%) on the dataset\n",
    "        train_data,final_test_data,train_labels,final_test_labels = utilities.data_split(data,labels,0.2)\n",
    "        \n",
    "        cross_valid_results = np.zeros((5,len(parameters)))\n",
    "    \n",
    "        ## then perform 5-k cross validation to find the best parameter on it\n",
    "        for k in range(5):\n",
    "            ## given a validation round split the training data into training and validation data of the round\n",
    "            validation_indexes,training_indexes = utilities.cross_validation(k,len(train_data),5)\n",
    "            round_train_data,round_train_labels,round_valid_data,round_valid_labels = utilities.get_train_test_set(validation_indexes,training_indexes,train_data,train_labels)\n",
    "            \n",
    "            data_per_class_dictionary = utilities.collect_each_class_images(round_train_data,round_train_labels,class_number)\n",
    "            pair_datasets = utilities.create_1_VS_1_dataset(data_per_class_dictionary)\n",
    "        \n",
    "            ## for every cross validation round check all the parameters\n",
    "            for i in range(0,len(parameters)):\n",
    "                param = parameters[i]\n",
    "                param_list = []\n",
    "                param_list = param_list + [kernel_type]\n",
    "                param_list  = param_list + [float(param)]\n",
    "                print(train_type + \" training for algorithm \" + algorithm + \" with params \" + str(param_list) +  \" random split \" + str(run_index+1) + \" and cross validation round: \" + str(k + 1))\n",
    "        \n",
    "                models_dict = None\n",
    "                pair_models = None\n",
    "                if(train_type == 'one_Vs_all'):\n",
    "                    models_dict = utilities.one_VS_all_training(data_per_class_dictionary,class_number,algorithm,param_list)\n",
    "                else:\n",
    "                    pair_models = utilities.one_vs_one_training(pair_datasets,algorithm,param_list)\n",
    "         \n",
    "                test_predictions = None\n",
    "                if(train_type == 'one_Vs_all'):\n",
    "                    test_predictions,test_confidence,average_confidence = utilities.one_VS_all_testing(algorithm,models_dict,round_valid_data,round_valid_labels)\n",
    "                else:\n",
    "                    test_predictions = utilities.one_vs_one_testing(algorithm,pair_models,round_valid_data,class_number)\n",
    "            \n",
    "                cross_valid_results[k][i] = utilities.calculate_accuracy(test_predictions,round_valid_labels)\n",
    "    \n",
    "        best_param_list = []\n",
    "        ## find the best parameter of this validation round\n",
    "        ## computing the average error for each parameter for each split\n",
    "        cross_valid_results = cross_valid_results.T\n",
    "        for i in range(cross_valid_results.shape[0]):\n",
    "            best_param_list.append((i,np.mean(cross_valid_results[i])))\n",
    "        \n",
    "        ## get the best result and the parameter for which the result achieved\n",
    "        best_param_list = sorted(best_param_list, key=lambda tup: tup[1],reverse = False)\n",
    "        \n",
    "        best_param = parameters[best_param_list[0][0]]\n",
    "        \n",
    "        print(\"Best Parameter for run round \" + str(run_index+1) + \" is \" + str(best_param) + \" with error: \" + str(best_param_list[0][1]))\n",
    "        \n",
    "        ## now retrain in full 80% dataset and test for the remaining hidden 20% test set\n",
    "        data_per_class_dictionary = utilities.collect_each_class_images(train_data,train_labels,class_number)\n",
    "        pair_datasets = utilities.create_1_VS_1_dataset(data_per_class_dictionary)\n",
    "        \n",
    "        param = best_param\n",
    "        param_list = []\n",
    "        param_list = param_list + [kernel_type]\n",
    "        param_list  = param_list + [float(param)]\n",
    "        print(train_type + \" training for algorithm \" + algorithm + \" with params \" + str(param_list) + \" for the full dataset\")\n",
    "        \n",
    "        models_dict = None\n",
    "        pair_models = None\n",
    "        if(train_type == 'one_Vs_all'):\n",
    "            models_dict = utilities.one_VS_all_training(data_per_class_dictionary,class_number,algorithm,param_list)\n",
    "        else:\n",
    "            pair_models = utilities.one_vs_one_training(pair_datasets,algorithm,param_list)\n",
    "         \n",
    "        ## test on the remaining 20% which was hidden through the cross validation\n",
    "        test_predictions = None\n",
    "        if(train_type == 'one_Vs_all'):\n",
    "            test_predictions,test_confidence,average_confidence = utilities.one_VS_all_testing(algorithm,models_dict,final_test_data,final_test_labels)\n",
    "        else:\n",
    "            test_predictions = utilities.one_vs_one_testing(algorithm,pair_models,final_test_data,class_number)\n",
    "         \n",
    "        accuracy = utilities.calculate_accuracy(test_predictions,final_test_labels)\n",
    "        results.append((accuracy,best_param))\n",
    "        \n",
    "        confusion_error_list.append(utilities.confusion_matrix(test_predictions,final_test_labels))\n",
    "        \n",
    "    print(results)\n",
    "    \n",
    "    print(\"--------\")\n",
    "    \n",
    "    utilities.get_average_confusions(confusion_error_list)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(EXPERIMENT_TYPE == 3):\n",
    "\n",
    "    data_per_class_dictionary = utilities.collect_each_class_images(data,labels,class_number)\n",
    "    param_list = []\n",
    "    param_list = param_list + [kernel_type]\n",
    "    param_list  = param_list + [float(parameters[1])]\n",
    "    models_dict = utilities.one_VS_all_training(data_per_class_dictionary,class_number,algorithm,param_list)\n",
    "    predictions,confidence,average_confidence = utilities.one_VS_all_testing(algorithm,models_dict,data,labels)\n",
    "\n",
    "\n",
    "    hard_items_indexes_1,hard_items_indexes_2 = utilities.get_hardest_to_predict_items(confidence,average_confidence,predictions,labels)\n",
    "    #print(hard_items_indexes_1)\n",
    "    #print(hard_items_indexes_2)\n",
    "    hard_images = []\n",
    "    hard_items_indexes = hard_items_indexes_1\n",
    "    for index in hard_items_indexes:\n",
    "        #imgplot = plt.imshow(data[index].reshape((16,16)))\n",
    "        #plt.show()\n",
    "        hard_images.append(data[index].reshape((16,16)))\n",
    "        \n",
    "    fig = plt.figure(figsize=(8., 8.))\n",
    "    grid = ImageGrid(fig, 111,nrows_ncols=(2,5),axes_pad=0.1)\n",
    "\n",
    "    for ax, im in zip(grid,hard_images):\n",
    "        ax.imshow(im)\n",
    "\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
