{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utilities\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import AdaBoost\n",
    "import KernelPerceptron\n",
    "%pylab inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "height = width = 16\n",
    "class_number = 10\n",
    "N = 5\n",
    "\n",
    "train_type = 'one_Vs_all'\n",
    "#train_type = 'one_Vs_one'\n",
    "\n",
    "#algorithm = 'AdaBoost'\n",
    "algorithm = 'KernelPerceptron'\n",
    "#algorithm = 'SVM'\n",
    "\n",
    "\n",
    "#kernel_type = 'gaussian'\n",
    "kernel_type = 'polynomial'\n",
    "#kernel_type = ' '\n",
    "\n",
    "parameters = [1,2,3,4,5,6,7]\n",
    "\n",
    "#parameters = [0.01,0.1,0.6,1,2,5]\n",
    "#parameters = [0.005,0.008,0.01,0.02,0.04]\n",
    "\n",
    "#parameters = [30,40,50,60,70]\n",
    "#parameters = [60,80,100]\n",
    "#parameters = [100,200,300]\n",
    "\n",
    "#parameters = [0.1,0.5,1,1.5,2,3,4,5,6]\n",
    "#parameters = [4,5,6]\n",
    "\n",
    "EXPERIMENT_TYPE = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = 'Datasets/full_dataset.dat'\n",
    "data,labels = utilities.read_data(train_data,width,height)\n",
    "print(data.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[0:1000]\n",
    "labels = labels[0:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#utilities.get_average_confusions(confusion_error_list)\n",
    "if(EXPERIMENT_TYPE == 1):\n",
    "    \n",
    "    ## N runs for random train test configurations\n",
    "    results = np.zeros((N,len(parameters)))\n",
    "    \n",
    "    confusion_error_list = []\n",
    "\n",
    "    for run_index in range(0,N):\n",
    "    \n",
    "        train_data,test_data,train_labels,test_labels,_ = utilities.data_split(data,labels,0.2)\n",
    "    \n",
    "        data_per_class_dictionary = utilities.collect_each_class_images(train_data,train_labels,class_number)\n",
    "        pair_datasets = utilities.create_1_VS_1_dataset(data_per_class_dictionary)\n",
    "        #print(\"Paired classifiers number: \" + str(len(pair_datasets)))\n",
    "\n",
    "        #print(train_data.shape)\n",
    "        #print(test_data.shape)\n",
    "        #print(train_labels.shape)\n",
    "        #print(test_labels.shape)\n",
    "        for i in range(0,len(parameters)):\n",
    "        \n",
    "            param = parameters[i]\n",
    "        \n",
    "            param_list = []\n",
    "            param_list = param_list + [kernel_type]\n",
    "            param_list  = param_list + [float(param)]\n",
    "            print(train_type + \" training for algorithm \" + algorithm + \" with params \" + str(param_list) +  \" and random split \" + str(run_index+1))\n",
    "        \n",
    "            models_dict = None\n",
    "            pair_models = None\n",
    "            if(train_type == 'one_Vs_all'):\n",
    "                models_dict = utilities.one_VS_all_training(data_per_class_dictionary,class_number,algorithm,param_list)\n",
    "            else:\n",
    "                pair_models = utilities.one_vs_one_training(pair_datasets,algorithm,param_list)\n",
    "         \n",
    "            test_predictions = None\n",
    "            if(train_type == 'one_Vs_all'):\n",
    "                test_predictions,test_confidence,average_confidence = utilities.one_VS_all_testing(algorithm,models_dict,test_data,test_labels)\n",
    "            else:\n",
    "                test_predictions = utilities.one_vs_one_testing(algorithm,pair_models,test_data,class_number)\n",
    "           \n",
    "            results[run_index][i] = utilities.calculate_accuracy(test_predictions,test_labels)\n",
    "            \n",
    "            confusion_error = utilities.confusion_matrix(test_predictions,test_labels)\n",
    "            \n",
    "            confusion_error_list.append(confusion_error)\n",
    "            \n",
    "            \n",
    "    for i in range(results.T.shape[0]):\n",
    "        print(\"Mean error and std for parameter: \" + str(parameters[i]) + \" is \" + str(np.mean(results.T[i])) + \" - \" + str(np.std(results.T[i])))\n",
    "        \n",
    "    conf_dict = utilities.get_average_confusions(confusion_error_list)\n",
    "    utilities.visualise_conf_dict(conf_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(EXPERIMENT_TYPE == 2):\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    confusion_error_list = []\n",
    "    \n",
    "    hardest_to_predict_dict = {}\n",
    "    \n",
    "    for run_index in range(0,N):\n",
    "        \n",
    "        print(\"------------------------------------------------\")\n",
    "        \n",
    "        ## make a random split (80%-20%) on the dataset\n",
    "        train_data,final_test_data,train_labels,final_test_labels, test_indexes = utilities.data_split(data,labels,0.2)\n",
    "        \n",
    "        cross_valid_results = np.zeros((5,len(parameters)))\n",
    "    \n",
    "        ## then perform 5-k cross validation to find the best parameter on it\n",
    "        for k in range(5):\n",
    "            ## given a validation round split the training data into training and validation data of the round\n",
    "            validation_indexes,training_indexes = utilities.cross_validation(k,len(train_data),5)\n",
    "            round_train_data,round_train_labels,round_valid_data,round_valid_labels = utilities.get_train_test_set(validation_indexes,training_indexes,train_data,train_labels)\n",
    "            \n",
    "            data_per_class_dictionary = utilities.collect_each_class_images(round_train_data,round_train_labels,class_number)\n",
    "            pair_datasets = utilities.create_1_VS_1_dataset(data_per_class_dictionary)\n",
    "        \n",
    "            ## for every cross validation round check all the parameters\n",
    "            for i in range(0,len(parameters)):\n",
    "                param = parameters[i]\n",
    "                param_list = []\n",
    "                param_list = param_list + [kernel_type]\n",
    "                param_list  = param_list + [float(param)]\n",
    "                #print(train_type + \" training for algorithm \" + algorithm + \" with params \" + str(param_list) +  \" random split \" + str(run_index+1) + \" and cross validation round: \" + str(k + 1))\n",
    "        \n",
    "                models_dict = None\n",
    "                pair_models = None\n",
    "                if(train_type == 'one_Vs_all'):\n",
    "                    models_dict = utilities.one_VS_all_training(data_per_class_dictionary,class_number,algorithm,param_list)\n",
    "                else:\n",
    "                    pair_models = utilities.one_vs_one_training(pair_datasets,algorithm,param_list)\n",
    "         \n",
    "                test_predictions = None\n",
    "                if(train_type == 'one_Vs_all'):\n",
    "                    test_predictions,test_confidence,average_confidence = utilities.one_VS_all_testing(algorithm,models_dict,round_valid_data,round_valid_labels)\n",
    "                else:\n",
    "                    test_predictions = utilities.one_vs_one_testing(algorithm,pair_models,round_valid_data,class_number)\n",
    "            \n",
    "                cross_valid_results[k][i] = utilities.calculate_accuracy(test_predictions,round_valid_labels)\n",
    "    \n",
    "        best_param_list = []\n",
    "        ## find the best parameter of this validation round\n",
    "        ## computing the average error for each parameter for each split\n",
    "        cross_valid_results = cross_valid_results.T\n",
    "        for i in range(cross_valid_results.shape[0]):\n",
    "            best_param_list.append((i,np.mean(cross_valid_results[i])))\n",
    "        \n",
    "        ## get the best result and the parameter for which the result achieved\n",
    "        best_param_list = sorted(best_param_list, key=lambda tup: tup[1],reverse = False)\n",
    "        \n",
    "        best_param = parameters[best_param_list[0][0]]\n",
    "        \n",
    "        #print(\"Best Parameter for run round \" + str(run_index+1) + \" is \" + str(best_param) + \" with error: \" + str(best_param_list[0][1]))\n",
    "        \n",
    "        ## now retrain in full 80% dataset and test for the remaining hidden 20% test set\n",
    "        data_per_class_dictionary = utilities.collect_each_class_images(train_data,train_labels,class_number)\n",
    "        pair_datasets = utilities.create_1_VS_1_dataset(data_per_class_dictionary)\n",
    "        \n",
    "        param = best_param\n",
    "        param_list = []\n",
    "        param_list = param_list + [kernel_type]\n",
    "        param_list  = param_list + [float(param)]\n",
    "        #print(train_type + \" training for algorithm \" + algorithm + \" with params \" + str(param_list) + \" for the full dataset\")\n",
    "        \n",
    "        models_dict = None\n",
    "        pair_models = None\n",
    "        if(train_type == 'one_Vs_all'):\n",
    "            models_dict = utilities.one_VS_all_training(data_per_class_dictionary,class_number,algorithm,param_list)\n",
    "        else:\n",
    "            pair_models = utilities.one_vs_one_training(pair_datasets,algorithm,param_list)\n",
    "         \n",
    "        ## test on the remaining 20% which was hidden through the cross validation\n",
    "        test_predictions = None\n",
    "        if(train_type == 'one_Vs_all'):\n",
    "            test_predictions,test_confidence,average_confidence = utilities.one_VS_all_testing(algorithm,models_dict,final_test_data,final_test_labels)\n",
    "        else:\n",
    "            test_predictions = utilities.one_vs_one_testing(algorithm,pair_models,final_test_data,class_number)\n",
    "         \n",
    "        accuracy = utilities.calculate_accuracy(test_predictions,final_test_labels)\n",
    "        results.append((accuracy,best_param))\n",
    "        \n",
    "        confusion_error_list.append(utilities.confusion_matrix(test_predictions,final_test_labels))\n",
    "        \n",
    "        if(train_type == 'one_Vs_all'):\n",
    "            hard_items_indexes,_ = utilities.get_hardest_to_predict_items(test_confidence,average_confidence,test_predictions,final_test_labels,test_indexes)\n",
    "\n",
    "            ## find the 5 hardest indexes of this round\n",
    "            hardest_to_predict_dict[run_index] = hard_items_indexes\n",
    "        \n",
    "            utilities.print_hard_items(hard_items_indexes,final_test_data)\n",
    "    \n",
    "    \n",
    "    ## based on the 5 hardest to be predicted itemss of every round\n",
    "    ## find the hardest to predict items overall\n",
    "    utilities.get_hardest_to_predict_items_overall(hardest_to_predict_dict,data)\n",
    "        \n",
    "    print(results)\n",
    "    \n",
    "    error = []\n",
    "    param = []\n",
    "    for (e,p) in results:\n",
    "        error.append(e)\n",
    "        param.append(p)\n",
    "        \n",
    "    print(\"--------\")\n",
    "\n",
    "        \n",
    "    print(\"Mean error: \" + str(np.mean(error)))\n",
    "    print(\"Std error: \" + str(np.std(error)))\n",
    "    print(\"Mean param: \" + str(np.mean(param)))\n",
    "    print(\"Std param: \" + str(np.std(param)))\n",
    "\n",
    "    \n",
    "    print(\"--------\")\n",
    "    \n",
    "    conf_dict = utilities.get_average_confusions(confusion_error_list)\n",
    "    \n",
    "    utilities.visualise_conf_dict(conf_dict)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgplot = plt.imshow(data[6253].reshape((16,16)))\n",
    "plt.show()\n",
    "print(labels[6253])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
